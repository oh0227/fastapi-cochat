import os
import asyncio
import multiprocessing as mp
from dotenv import load_dotenv
from openai import OpenAI
from schemas import MessageBase

from langchain_unstructured import UnstructuredLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma  # âœ… ìµœì‹  ë°©ì‹
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

from duckduckgo_search import DDGS
from newspaper import Article

# âœ… Macì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ ë°©ì§€
mp.set_start_method("fork", force=True)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)
GPT_MODEL = "gpt-4o-mini"

# âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"batch_size": 1, "normalize_embeddings": False}
)

# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
vector_store = Chroma(
    embedding_function=embedding_model,
    persist_directory="./chroma_db"
)

# âœ… GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_with_gpt(message):
    prompt = f"""
ë©”ì‹œì§€ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ(ê³ ìœ  ëª…ì‚¬, ì€ì–´, ì¸ë¬¼, ì œí’ˆ ì´ë¦„ ë“±)ë¥¼ ì¶”ì¶œí•´ì¤˜.
ë©”ì‹œì§€: {message}

ê²°ê³¼ëŠ” íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•´ì¤˜.
ì˜ˆ: ["ì†Œí¬ë¼í…ŒìŠ¤", "ìƒë¬´ë‹˜"]
"""
    try:
        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return eval(response.choices[0].message.content)
    except Exception as e:
        print("â— í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨:", e)
        return []

# âœ… DuckDuckGo ê²€ìƒ‰
def search_web_pages(query, max_results=3):
    with DDGS() as ddgs:
        return [r["href"] for r in ddgs.text(query, max_results=max_results)]

# âœ… ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
def extract_text_from_url(url):
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        print("â— URL ì¶”ì¶œ ì‹¤íŒ¨:", e)
        return ""

# âœ… ë¬¸ì„œ ì„ë² ë”© ë° ì¶”ê°€
def add_docs_to_vectorstore(texts, source_name):
    new_docs = [Document(page_content=t, metadata={"source": source_name}) for t in texts]
    new_docs = filter_complex_metadata(new_docs)
    vector_store.add_documents(new_docs)

# âœ… ì›¹ ê¸°ë°˜ ë¬¸ì„œ ìë™ ì¶”ê°€
def expand_rag_with_web(message_content):
    keywords = extract_keywords_with_gpt(message_content)
    print("ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ:", keywords)

    all_texts = []
    for kw in keywords:
        urls = search_web_pages(kw)
        print(f"ğŸ”— {kw} ê´€ë ¨ URL:", urls)
        for url in urls:
            text = extract_text_from_url(url)
            if text:
                all_texts.append(text)

    if all_texts:
        add_docs_to_vectorstore(all_texts, "web")
        print(f"âœ… {len(all_texts)} ê°œ ì›¹ ë¬¸ì„œ ì¶”ê°€ë¨.")
    else:
        print("âš ï¸ ì›¹ì—ì„œ ìœ ì˜ë¯¸í•œ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# âœ… ë©”ì‹œì§€ ì²˜ë¦¬ RAG + GPT
def process_message(message: MessageBase):
    content = f"""
    Sender: {message.sender_id}
    Subject: {message.subject}
    Content: {message.content}
    """

    try:
        results = vector_store.similarity_search(message.content, k=5)
    except Exception as e:
        print("â— Similarity search failed:", e)
        results = []

    contexts = " ".join([r.page_content for r in results]) if results else "No context found."

    system_prompt = """\
You are an assistant that processes internal messages.
Your tasks:
1. Extract important keywords from the message (especially slangs or named entities).
2. Use context to understand any unclear terms.
3. Classify the message into one of the categories: "deadline", "payment", "public", "office", "others".
4. Return output as JSON with fields: keywords, summary, category.
"""

    user_prompt = f"""
Message Metadata and Content:
{content}

Context for understanding:
{contexts}

Respond in the following format:
{{
  "keywords": [...],
  "summary": "...",
  "category": "..."
}}
"""

    try:
        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        print("â— OpenAI API call failed:", e)
        return "{}"

# âœ… ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸
def run_rag_pipeline(message: MessageBase):
    try:
        expand_rag_with_web(message.content)
        return process_message(message)
    except Exception as e:
        print("â— ì „ì²´ RAG ì²˜ë¦¬ ì‹¤íŒ¨:", e)
        return "{}"